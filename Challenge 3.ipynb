{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 3\n",
    "\n",
    "### The question\n",
    "For challenge 3 we are tasked to compare machine learning algorithms and see which ever one is best. Within this notebook we are going to be trying out; SVM, MLP, Random Forest and Essembly Learning. We will comparing the accuracy and seeing which algorithm is the best for this dataset.\n",
    "\n",
    "### Optimizing the algorithms\n",
    "For each algorithm we will be trying to find the best possible options to get the best result out of each algorithm.\n",
    "\n",
    "### Personal goals\n",
    "#### Moduality\n",
    "This has also been applied in challenge 1, I want to make all my methods as modular as possible. This means having my own methods have plenty of parameters to edit the SKLearn methods.\n",
    "#### Documentation\n",
    "I want to document everything as well as I possibly can. Making comments on code lines and having markdown sections to explain my reasoning behind the code as well as I can. This will be the biggest challenge for me."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The dataset\n",
    "The dataset I picked for this assignment was ***DATASET HERE*** It mostly peaked my personal interest and seemed easy enough to work with. I found this dataset on *Keggle* and [***the this is the link to the dataset***](http://google.com).\n",
    "The domain of this dataset is ***DOMAIN*** because...\n",
    "\n",
    "\n",
    "### Description\n",
    "......\n",
    "### Columns\n",
    "....\n",
    "### Label and goal\n",
    "....."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepairing the notebook\n",
    "To start off this notebook we are going to be making methods. We make these to prepare for a dataset which might change. As this is the first dataset I need to pick myself, it might be a unusable dataset with features that are not good enough to use in this context. With the methods made I can simply change out the dataset and everything should still work.\n",
    "\n",
    "Since Katherine and I also did this for challenge 1 to be able to swap out the Iris and Whine set easily, I will be getting my code from the challenge 1. This will be noted in the documentation but will still be explained.\n",
    "### Importing libraries\n",
    "We will be using a lot of libraries within this notebook and I will document these all in this code section. There should be no import throughout the notebook itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version: 1.12.1\n",
      "Pandas version: 0.20.1\n",
      "Scikit-learn version: 0.18.1\n",
      "Matplotlib version: 2.0.2\n"
     ]
    }
   ],
   "source": [
    "#Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "#Further imports\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "#Versionprinting\n",
    "print('NumPy version:', np.__version__)\n",
    "print('Pandas version:', pd.__version__)\n",
    "print('Scikit-learn version:', sk.__version__)\n",
    "print('Matplotlib version:', matplotlib.__version__)\n",
    "#Inline version\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "### Boxplots\n",
    "To analyze our features we want to look at the features and see how they compare. What features are good to use and where do we see the least amount of overlap. To make this easier on ourselves, we are going to write an automated method to do this work. We can just make a list of features and run these through the method and out come all the boxplots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def boxPlots(df, features, label):\n",
    "    if(len(features)>4):\n",
    "        for i in range(int(len(features)/4)+1):\n",
    "            try:\n",
    "                df.boxplot(column=features[4*i:4*(i+1)], by=label, figsize=(15,8), layout=(1,4))\n",
    "            except:\n",
    "                try:\n",
    "                    df.boxplot(column=features[4*i], by=label, figsize=(15,8), layout=(1,4))\n",
    "                    df.boxplot(column=features[4*i+1], by=label, figsize=(15,8), layout=(1,4))\n",
    "                    df.boxplot(column=features[4*i+2], by=label, figsize=(15,8), layout=(1,4))\n",
    "                    df.boxplot(column=features[4*i+3], by=label, figsize=(15,8), layout=(1,4))\n",
    "                except:\n",
    "                    df = df #Index is out of range but needs code to not go further\n",
    "    else:\n",
    "        df.boxplot(column=features, by=label, figsize=(15,8), layout=(1,4))\n",
    "#         maximums = []\n",
    "#         for i in range(len(features)):\n",
    "#             maximums.append(max(df[features[i]]))\n",
    "#         for i in range(int(len(features)/4)+1):\n",
    "#             seperate=False\n",
    "#             seperateindex = 0\n",
    "#             beginindex = 4*i\n",
    "#             endindex= 4*(i+1)\n",
    "#             equalizer = 5\n",
    "#             plotted = features[beginindex:endindex]\n",
    "#             try:\n",
    "#                 for q in range(4):\n",
    "#                     if(int(maximums[beginindex+q]/maximums[beginindex]) > equalizer) or (int(maximums[beginindex+q]/maximums[beginindex+1]) > equalizer) or(int(maximums[beginindex+q]/maximums[beginindex+2]) > equalizer) or (int(maximums[beginindex+q]/maximums[beginindex+3]) > equalizer):\n",
    "#                         seperate = True\n",
    "#                         seperateindex = q\n",
    "#             except:\n",
    "#                 seperate = True \n",
    "#             if(seperate == False):\n",
    "#                 df.boxplot(column=plotted, by=label, figsize=(15,8), layout=(1,4))\n",
    "#             else:\n",
    "#                 rest = plotted[beginindex:seperateindex] + plotted[seperateindex+1:endindex]\n",
    "#                 print(\"Index: \" + str(seperateindex))\n",
    "#                 print(len(plotted[beginindex:seperateindex]))\n",
    "#                 print(len(plotted[seperateindex+1:endindex]))\n",
    "#                 print(len(rest))\n",
    "#                 df.boxplot(column=plotted[seperateindex], by=label, figsize=(15,8), layout=(1,4))\n",
    "#                 df.boxplot(column=rest, by=label, figsize=(15,8), layout=(1,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explaination\n",
    "### Defining the method\n",
    "To start off, we give the method the name `boxPlots` it is descriptive enough for me and easy to type.\n",
    "#### The parameters\n",
    "    - df:       The dataframe used to visualize the features\n",
    "    - features: A Tuple of the features wanting to be analysed\n",
    "    - label:    The string of which column is the label\n",
    "```py\n",
    "def boxPlots(df, features, label):\n",
    "```\n",
    "### Checking the amount of features present\n",
    "If we have more than 4 features we want to split them up in to multiple sets of 4. This is why we do an initial check to see if we have more than 4 features.\n",
    "```py\n",
    "if(len(features)>4):\n",
    "```\n",
    "### Printing the boxplots by 4\n",
    "If there are more features than 4 we want to split these into groups of 4 and print them seperately.\n",
    "First, we make a loop by deviding the amount of features by 4 and adding 1. The `int()` method **ALWAYS** rounds downwards and this is not good for us as the last features will be cut off. This is the reason we add 1.\n",
    "Second, we want to try plotting all 4 figures at once, if this fails it is most likely an index out of range exception. To make sure all boxplots get plotted we then again implement a try in the catch to plot every single boxplot after one another.\n",
    "This isn't the most clean way to do it but it works.\n",
    "```py\n",
    "        for i in range(int(len(features)/4)+1):\n",
    "            try:\n",
    "                df.boxplot(column=features[4*i:4*(i+1)], by=label, figsize=(15,8), layout=(1,4))\n",
    "            except:\n",
    "                try:\n",
    "                    df.boxplot(column=features[4*i], by=label, figsize=(15,8), layout=(1,4))\n",
    "                    df.boxplot(column=features[4*i+1], by=label, figsize=(15,8), layout=(1,4))\n",
    "                    df.boxplot(column=features[4*i+2], by=label, figsize=(15,8), layout=(1,4))\n",
    "                    df.boxplot(column=features[4*i+3], by=label, figsize=(15,8), layout=(1,4))\n",
    "                except:\n",
    "                    df = df #Index is out of range but needs code to not go further\n",
    "                    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter plots\n",
    "We can also use scatter plots to see how the features are seperated from each other. This is a very clear and visual representation of these points. Again we would like to automate things to make it easy to work with our dataset and swap features if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scatterplots(features, colors, labelnames, splitsize, random):    \n",
    "    for value in features:\n",
    "        names = list(value)\n",
    "        value = StandardScaler().fit(value).transform(value)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(value,y,test_size=splitsize, random_state=random)\n",
    "        for i in range(len(colors)):\n",
    "            x1s = X_train[:, 0][y_train.as_matrix() == i+1]\n",
    "            x2s = X_train[:, 1][y_train.as_matrix() == i+1]\n",
    "            plt.scatter(x1s, x2s, c=colors[i])\n",
    "\n",
    "        plt.legend(labelnames)\n",
    "        plt.xlabel(\"Normalized \" + names[0])\n",
    "        plt.ylabel(\"Normalized \" + names[1]);\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KBest in a bar plot\n",
    "KBest is made by SKLearn itself to search for the best features for the kNN algorithm. I decided to add this as a little extra since it has a clearn design. You can easily see which features are seen as the best by SKLearn and if that goes along with what we have found through our Box and Scatter plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findKBestFeatures(df, features, label):\n",
    "    selector = SelectKBest(f_classif, k=5)\n",
    "    selector.fit(df[features], df[label])\n",
    "    scores = -np.log10(selector.pvalues_)\n",
    "    plt.bar(range(len(predictors)), scores)\n",
    "    plt.xticks(range(lenpredictors), predictors, rotation=\"vertical\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning\n",
    "## Accuracy tests\n",
    "We want to test our data as efficient and modular as possible. I started noticing a pattern within the machine learning algorithms, you always define the classifier and then just use `clf.fit(X_train, y_train)`, `y_pred = clf.predict(x_test)` and `accuracy_score(y_test, y_pred)`. This means I can store these classifiers in a list and simpely loop over them instead of having to call the same sort of method around 12 times.\n",
    "\n",
    "The make it easier to read I have provided the optional parameter `methodnames` so I can give a list of string that corrospond to the right method that was used. So instead of only printing out simple percentages, we get a nice summery in something like this:\n",
    "*SVM: 96.46942%*\n",
    "*Neural Network 1 Layer: 60.1684%*\n",
    "*Neural Network 2 Layers: 86.6654%*\n",
    "\n",
    "This gives us a nice and clean overview of what has been done with what method and removes the otherwise cluthered mess.\n",
    "I could have included the normalizing and train test splitting within this method but decided not to do so for simplicity.\n",
    "Having to explain a method like that would take a lot of space and I would prefer to keep that seperated in the datacleaning section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(methods, X_train, X_test, y_train, y_test,methodnames):\n",
    "    for i in len(methods):\n",
    "        methods[i].fit(X_train, y_train)\n",
    "        y_pred = methods[i].predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        if(len(methodnames) > 0) and (len(methodnames)== len(methods)):\n",
    "            print(methodnames[i] +\": \"+ str(accuracy*100) + \"%\")\n",
    "        else:\n",
    "            print(str(accuracy*100)+\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepairing for the accuracy tests\n",
    "Because the method does a lot by itself we only need to make our classifiers and add these to a list.\n",
    "I am going to prepare 2 lists, one with methods and one with methodnames.\n",
    "For the tests I will be trying out a lot of options within the parameters of the classifiers. These will all be noted in the methodnames to keep things organized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "methods = []\n",
    "methodnames = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods.append(SVC(kernel='linear', C=5))\n",
    "methods.append(SVC(kernel='linear', C=10))\n",
    "methods.append(SVC(kernel='linear', C=100))\n",
    "methodnames.append(\"Linear SVC with C=5\")\n",
    "methodnames.append(\"Linear SVC with C=10\")\n",
    "methodnames.append(\"Linear SVC with C=100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "methods.append(tree.DecisionTreeClassifier(criterion = 'entropy', max_depth = 3, min_samples_leaf = 5))\n",
    "methods.append(tree.DecisionTreeClassifier(criterion = 'entropy', max_depth = 5, min_samples_leaf = 5))\n",
    "methods.append(tree.DecisionTreeClassifier(criterion = 'entropy', max_depth = 7, min_samples_leaf = 5))\n",
    "methodnames.append(\"Decision Tree with Depth 3 and Minimun samples leaf 5\")\n",
    "methodnames.append(\"Decision Tree with Depth 5 and Minimun samples leaf 5\")\n",
    "methodnames.append(\"Decision Tree with Depth 7 and Minimun samples leaf 5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "methods.append(MLPClassifier(hidden_layer_sizes=1))\n",
    "methods.append(MLPClassifier(hidden_layer_sizes=2))\n",
    "methods.append(MLPClassifier(hidden_layer_sizes=3))\n",
    "methodnames.append(\"MLP Neural Network with 1 hidden layer\")\n",
    "methodnames.append(\"MLP Neural Network with 2 hidden layers\")\n",
    "methodnames.append(\"MLP Neural Network with 3 hidden layers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Controlling the lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Linear SVC with C=5', 'Linear SVC with C=10', 'Linear SVC with C=100', 'Linear SVC with C=5', 'Linear SVC with C=10', 'Linear SVC with C=100', 'Decision Tree with Depth 3 and Minimun samples leaf 5', 'Decision Tree with Depth 5 and Minimun samples leaf 5', 'Decision Tree with Depth 7 and Minimun samples leaf 5', 'MLP Neural Network with 1 hidden layer', 'MLP Neural Network with 2 hidden layers', 'MLP Neural Network with 3 hidden layers']\n",
      "Amount of methods: 12\n",
      "Amount of names: 12\n",
      "Correct amount of method names\n"
     ]
    }
   ],
   "source": [
    "print(methodnames)\n",
    "print(\"Amount of methods: \" + str(len(methods)))\n",
    "print(\"Amount of names: \" + str(len(methodnames)))\n",
    "if(len(methods)==len(methodnames)):\n",
    "    print(\"Correct amount of method names\")\n",
    "elif(methodnames == 0):\n",
    "    print(\"No method names supplied\")\n",
    "else:\n",
    "    print(\"Wrong amount of method names supplied\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
